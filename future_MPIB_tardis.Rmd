---
title: "try"
author: "Ruben Arslan"
date: "30 October 2017"
output: 
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Future package
Vignettes in package ‘future’:

```{r load}
# vignette("future-1-overview")
library(future)
```

## Use R directly on the server (no scheduler)

This approach works for a server that is SSH-accessible and has R and the __future__ package installed. To be able to SSH in without storing your password in plaintext somewhere, you'll need to set up public key authentification. You do this by copying your public key to `~/.ssh/authorized_keys` on the server. If you've already generated a public key on your local machine, it should be in `~/.ssh/id_rsa.pub` or something similar. If you haven't yet, Google how :-)
You'll have to do this for every server you want to use.

You can automatically do this using ([source](https://askubuntu.com/questions/4830/easiest-way-to-copy-ssh-keys-to-another-machine):

```bash
ssh-copy-id user@tardis.mpib-berlin.mpg.de
```



```{r}
library("future") # this is the package we use to flexibly scale our processing
library("listenv") # this is a helper package that allows us to assign the results to a list even if they come in at different times

## Set up access to remote login node
login <- tweak(remote, workers = "arslan@tardis.mpib-berlin.mpg.de") # Your username goes before the @
plan(login) # this tells the future package that we want to use a remote server for running our future operations

# but maybe the reason we want to shift to a server is not that we want to run a simple sequential operation there.
# maybe we want to take advantage of parallelisation and run the operation in parallel on multiple cores
## Specify future topology
## login node -> { cluster nodes } -> { multiple cores }
plan(list(
  login,
  multicore,
  sequential
))
# demo("mandelbrot", package = "future", ask = FALSE)


## (a) This will be evaluated on the cluster login computer
x %<-% { # this operator binds the result of the promise/future to x
  thost <- Sys.info()[["nodename"]]
  tpid <- Sys.getpid()
  y <- listenv()
  for (task in 1:4) {
    ## (b) This will be evaluated on a different core
    y[[task]] %<-% {
      mhost <- Sys.info()[["nodename"]]
      mpid <- Sys.getpid()
      z <- listenv()
      for (jj in 1:2) {
        ## (c) this will be evaluated sequentially on the same core
        z[[jj]] %<-% data.frame(task = task,
                                top.host = thost, top.pid = tpid,
                                mid.host = mhost, mid.pid = mpid,
                                host = Sys.info()[["nodename"]],
                                pid = Sys.getpid())
      }
      Reduce(rbind, z)
    }
  }
  Reduce(rbind, y)
}

x
```


## TARDIS cluster
Of course if you want to do something more major on the cluster, you need to use the Torque job scheduler.
For this, you need a second package `future.batchtools`. Further, you need to copy a template file for job scripts to the server.

```{r}
library("future")
library("listenv")
library("future.batchtools")
library(debugme)
Sys.setenv(DEBUGME='batchtools')
library(batchtools)

## Set up access to remote login node
login <- tweak(remote, workers = "arslan@tardis.mpib-berlin.mpg.de")
qsub <- tweak(batchtools_torque, template = 'torque-lido.tmpl', 
            # workers = "export LSF_ENVDIR=/opt/lsf/conf",
                resources = list(job.name = 'test1',
                                queue = 'default',
                                walltime = "0:1:0", # 1 minute runtime
                                memory = 1,
                                processes = 4))


# we take our local template and write it to the server
plan(login)
tmpl = readLines("torque-lido.tmpl")
value(future({   cat(tmpl, file = "torque-lido.tmpl", sep = "\n") }))
all.equal(tmpl, value(future({   readLines("torque-lido.tmpl") })))


## Specify future topology
## login node -> { cluster nodes } -> { multiple cores }
plan(list(
  login,
  qsub,
  multicore
))



x %<-% {
  thost <- Sys.info()[["nodename"]]
  tpid <- Sys.getpid()
  # transfer my local tmpl file
  y <- listenv()
  for (task in 1:4) {
    ## (b) This will be evaluated on a compute node on the cluster
    y[[task]] %<-% {
      mhost <- Sys.info()[["nodename"]]
      mpid <- Sys.getpid()
      z <- listenv()
      for (jj in 1:2) {
        ## (c) These will be evaluated in separate processes on the same compute node
        z[[jj]] %<-% data.frame(task = task,
                                top.host = thost, top.pid = tpid,
                                mid.host = mhost, mid.pid = mpid,
                                host = Sys.info()[["nodename"]],
                                pid = Sys.getpid())
      }
      Reduce(rbind, z)
    }
  }
  Reduce(rbind, y)
}

x
```

## brms
brms is a Bayesian regression modelling software. Because we often run multiple chains in brms to assess convergence and obtain samples, more quickly, parallelisation is beneficial. brms has internal support for futures. By setting its `future` argument to true, it will run the chains according to `plan`.
Because running such a model can take a lot of time, it is beneficial to offload it to a remote server or cluster.

## use brms on Tardis

```{r}

## Set up access to remote login node
# login_vpn <- tweak(remote, workers = "rarslan@login.gwdg.de") # doesn't work because R not installed
login <- tweak(remote, workers = "arslan@tardis.mpib-berlin.mpg.de")
qsub <- tweak(batchtools_torque, template = 'torque-lido.tmpl', 
            # workers = "export LSF_ENVDIR=/opt/lsf/conf",
                resources = list(job.name = 'test1',
                                queue = 'default',
                                walltime = "1:0:0", # 1 hour runtime
                                memory = 4,
                                processes = 4))

library(brms)

## Specify future topology
## login node -> { cluster node (compile brms model) } -> { run chains on multiple cores }
plan(list(
  login,
  qsub,
  multicore
))

mydata = data.frame(x = rnorm(100), y = rnorm(100))

model_name <- "mydata_model.rds"
fit2 %<-% { # retrieve the model if it has been fit already
  if (file.exists(model_name))
    readRDS(model_name)
}
if (is.null(fit2)) {
  fit2 %<-% { # login to tardis
    model %<-% { # qsub
      message(model_name)
      brm(x ~ y, 
        data = mydata, 
        chains = 4,
        future = TRUE #multicore
        )
    }
    saveRDS(model, file = model_name) # in case we lose the connection, save the result
    model
  }
}

# but the program will wait for it
fit2

# don't make your plots on the cluster, make em at home
marginal_effects(fit2)
```


You can track progress for your job here:
https://tardis.mpib-berlin.mpg.de/jobs

(find your name, click the job)



## local env
```{r}
sessionInfo()
```

## remote env
```{r comment=""}
plan(login)

value(future({sessionInfo() }))
```

### .profile
```{r comment=""}
cat(value(future({ readr::read_file(".profile") })))
```

### torque-lido.tmpl
```{r comment=""}
cat(value(future({ readr::read_file("torque-lido.tmpl") })))
```

