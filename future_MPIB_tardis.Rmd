---
title: "try"
author: "Ruben Arslan"
date: "30 October 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Future package
Vignettes in package ‘future’:

```{r load}
# vignette("future-1-overview")
library(future)
```

## Simple, local

A simple local example. Multiprocess starts whatever multiprocess tool is available on the platform (e.g. Multicore on Unix-based system, PSOCK cluster on Windows).
```{r}
plan(multiprocess)
v %<-% {
 cat("Resolving...\n")
 3.14
}
v
```

## Use R directly on the server (no scheduler)

This approach works for a server that is SSH-accessible and has R and the __future__ package installed. To be able to SSH in without storing your password in plaintext somewhere, you'll need to set up public key authentification. You do this by copying your public key to `~/.ssh/authorized_keys` on the server. If you've already generated a public key on your local machine, it should be in `~/.ssh/id_rsa.pub` or something similar. If you haven't yet, Google how :-)
You'll have to do this for every server you want to use.

```{r}
library("future") # this is the package we use to flexibly scale our processing
library("listenv") # this is a helper package that allows us to assign the results to a list even if they come in at different times

## Set up access to remote login node
login <- tweak(remote, workers = "arslan@tardis.mpib-berlin.mpg.de") # Your username goes before the @
plan(login) # this tells the future package that we want to use a remote server for running our future operations

# but maybe the reason we want to shift to a server is not that we want to run a simple sequential operation there.
# maybe we want to take advantage of parallelisation and run the operation in parallel on multiple cores
## Specify future topology
## login node -> { cluster nodes } -> { multiple cores }
plan(list(
  login,
  multicore,
  sequential
))
# demo("mandelbrot", package = "future", ask = FALSE)


## (a) This will be evaluated on the cluster login computer
x %<-% { # this operator binds the result of the promise/future to x
  thost <- Sys.info()[["nodename"]]
  tpid <- Sys.getpid()
  y <- listenv()
  for (task in 1:4) {
    ## (b) This will be evaluated on a different core
    y[[task]] %<-% {
      mhost <- Sys.info()[["nodename"]]
      mpid <- Sys.getpid()
      z <- listenv()
      for (jj in 1:2) {
        ## (c) this will be evaluated sequentially on the same core
        z[[jj]] %<-% data.frame(task = task,
                                top.host = thost, top.pid = tpid,
                                mid.host = mhost, mid.pid = mpid,
                                host = Sys.info()[["nodename"]],
                                pid = Sys.getpid())
      }
      Reduce(rbind, z)
    }
  }
  Reduce(rbind, y)
}

print(x)
```


## TARDIS cluster
Of course if you want to do something more major on the cluster, you need to use the Torque job scheduler.
For this, you need a second package `future.batchtools`. Further, you need to copy a template file for job scripts to the server.

```{r}
library("future")
library("listenv")
library("future.batchtools")
library(debugme)
Sys.setenv(DEBUGME='batchtools')
library(batchtools)

## Set up access to remote login node
login <- tweak(remote, workers = "arslan@tardis.mpib-berlin.mpg.de")
qsub <- tweak(batchtools_torque, template = 'torque-lido.tmpl', 
            # workers = "export LSF_ENVDIR=/opt/lsf/conf",
                resources = list(job.name = 'test1',
                                log.file = 'blaaa2.log',
                                queue = 'default',
                                walltime = 10 * 60,
                                memory = 1*1024,
                                processes = 4))

## Specify future topology
## login node -> { cluster nodes } -> { multiple cores }
plan(list(
  login,
  qsub,
  multicore
))

tmpl = readLines("torque-lido.tmpl")
x %<-% {
  thost <- Sys.info()[["nodename"]]
  tpid <- Sys.getpid()
  # transfer my local tmpl file
  cat(tmpl, file = "torque-lido.tmpl", sep = "\n")
  y <- listenv()
  for (task in 1:4) {
    ## (b) This will be evaluated on a compute node on the cluster
    y[[task]] %<-% {
      mhost <- Sys.info()[["nodename"]]
      mpid <- Sys.getpid()
      z <- listenv()
      for (jj in 1:2) {
        ## (c) These will be evaluated in separate processes on the same compute node
        z[[jj]] %<-% data.frame(task = task,
                                top.host = thost, top.pid = tpid,
                                mid.host = mhost, mid.pid = mpid,
                                host = Sys.info()[["nodename"]],
                                pid = Sys.getpid())
      }
      Reduce(rbind, z)
    }
  }
  Reduce(rbind, y)
}

print(x)

```

## brms
brms is a Bayesian regression modelling software. Because we often run multiple chains in brms to assess convergence and obtain samples, more quickly, parallelisation is beneficial. brms has internal support for futures. By setting its `future` argument to true, it will run the chains according to `plan`.
Because running such a model can take a lot of time, it is beneficial to offload it to a remote server or cluster.

## use brms on Tardis

```{r}

## Set up access to remote login node
# login_vpn <- tweak(remote, workers = "rarslan@login.gwdg.de") # doesn't work because R not installed
login <- tweak(remote, workers = "arslan@tardis.mpib-berlin.mpg.de")
qsub <- tweak(batchtools_torque, template = 'torque-lido.tmpl', 
            # workers = "export LSF_ENVDIR=/opt/lsf/conf",
                resources = list(job.name = 'test1',
                                log.file = 'blaaa2.log',
                                queue = 'default',
                                walltime = 10 * 60,
                                memory = 10*1024,
                                processes = 4))
library(brms)

## Specify future topology
## login node -> { cluster node (compile brms model) } -> { run chains on multiple cores }
plan(list(
  login,
  qsub,
  multicore
))

fit2 %<-% { # login to tardis
  x %<-% { # bsub
  library("brms")
  brm(count ~ log_Age_c + log_Base4_c * Trt + (1|patient) + (1|obs), 
      data = epilepsy, family = poisson(), 
      chains = 4,
      prior = c(prior(student_t(5,0,10), class = b),
      prior(cauchy(0,2), class = sd)), 
      future = TRUE #multicore
      )
    } %globals% FALSE
  x
  }

fit2
```

### use brms with my own data on Tardis
Also showing a different way to set up the future and pass along globals and packages.

```{r}
## Specify future topology
## login node -> { cluster nodes } -> { multiple cores }
plan(list(
  login,
  qsub,
  multicore
))

mydata = data.frame(x = rnorm(100), y = rnorm(100))

fit2 = future({
  # login to tardis
  # then send off bsub and wait for result
  value(future({ # bsub
  brm(x ~ y, 
      data = mydata, 
      chains = 4,
      future = TRUE #multicore
      )
    }, globals = c("mydata"), packages = 'brms'
  ))
})

resolved(fit2)
# while (!resolved(fit2)) {
#   Sys.sleep(1)
# }
summary(value(fit2))
```



