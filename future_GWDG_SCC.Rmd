---
title: "try"
author: "Ruben Arslan"
date: "30 October 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Future package

```{r load}
# vignette("future-1-overview")
library(future)
```

## GWDG cluster
To run jobs on a cluster, you need to use a job scheduler. For this, you need a second package `future.batchtools`. Further, you need to copy a template file for job scripts to the server.

```{r}
library("future")
library("listenv")
library("future.batchtools")
library(debugme)
Sys.setenv(DEBUGME='batchtools')
library(batchtools)

## Set up access to remote login node
# login_vpn <- tweak(remote, workers = "rarslan@login.gwdg.de") # doesn't work because R not installed
login <- tweak(remote, workers = "gwdu102.gwdg.de", user = 'rarslan')
bsub <- tweak(batchtools_lsf, template = 'lsf.tmpl', 
            # workers = "export LSF_ENVDIR=/opt/lsf/conf",
                resources = list(job.name = 'test1',
                                log.file = 'blaaa2.log',
                                queue = 'mpi',
                                walltime = '1:00',
                                processes = 4))

## Specify future topology
## login node -> { cluster nodes } -> { multiple cores }
plan(list(
  login,
  bsub,
  multicore
))
# demo("mandelbrot", package = "future", ask = FALSE)

x %<-% {
  thost <- Sys.info()[["nodename"]]
  tpid <- Sys.getpid()
  # set_gwdg_env()
  y <- listenv()
  for (task in 1:4) {
    ## (b) This will be evaluated on a compute node on the cluster
    y[[task]] %<-% {
      mhost <- Sys.info()[["nodename"]]
      mpid <- Sys.getpid()
      z <- listenv()
      for (jj in 1:2) {
        ## (c) These will be evaluated in separate processes on the same compute node
        z[[jj]] %<-% data.frame(task = task,
                                top.host = thost, top.pid = tpid,
                                mid.host = mhost, mid.pid = mpid,
                                host = Sys.info()[["nodename"]],
                                pid = Sys.getpid())
      }
      Reduce(rbind, z)
    }
  }
  Reduce(rbind, y)
}

print(x)

```

## brms
brms is a Bayesian regression modelling software. Because we often run multiple chains in brms to assess convergence and obtain samples, more quickly, parallelisation is beneficial. brms has internal support for futures. By setting its `future` argument to true, it will run the chains according to `plan`.
Because running such a model can take a lot of time, it is beneficial to offload it to a remote server or cluster.

### use brms with my own data on LSF
Also showing a different way to set up the future and pass along globals and packages.

```{r}
login <- tweak(remote, workers = "gwdu102.gwdg.de", user = 'rarslan')
bsub <- tweak(batchtools_lsf, template = 'lsf.tmpl', 
            # workers = "export LSF_ENVDIR=/opt/lsf/conf",
                resources = list(job.name = 'brms_test',
                                log.file = 'brms_test.log',
                                queue = 'mpi',
                                walltime = '2:00',
                                processes = 4))
library(brms)

## Specify future topology
## login node -> { cluster node (compile brms model) } -> { run chains on multiple cores }
plan(list(
  login,
  bsub,
  multicore
))


mydata = data.frame(x = rnorm(100), y = rnorm(100))

model_name <- "mydata_model.rds"
fit2 %<-% { # retrieve the model if it has been fit already
  if (file.exists(model_name))
    readRDS(model_name)
}
if (is.null(fit2)) {
  fit2 %<-% { # login to tardis
    model %<-% { # qsub
      message(model_name)
      brm(x ~ y, 
        data = mydata, 
        chains = 4,
        future = TRUE #multicore
        )
    }
    saveRDS(model, file = model_name) # in case we lose the connection, save the result
    model
  }
}

# but the program will wait for it
fit2

# don't make your plots on the cluster, make em at home
marginal_effects(fit2)
```

## local env
```{r}
sessionInfo()
```

## remote env
```{r}
plan(login)

value(future({sessionInfo() }))
```

